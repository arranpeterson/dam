{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "Configures the libraries for Amazon Bedrock, Amazon OpenSearch Serverless, and visualization libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3>=1.28.57\n",
    "!pip install awscli>=1.29.57\n",
    "!pip install botocore>=1.31.57\n",
    "!pip install opensearch-py==2.3.1\n",
    "!pip install pypdf>=3.8,<4\n",
    "!pip install matplotlib==3.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install boto3\n",
    "!python3 -m pip install awscli\n",
    "!python3 -m pip install botocore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a CloudFront for the image_url \n",
    "Create a cloudfront URL for the images to be accessible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import boto3\n",
    "os.environ['AWS_PROFILE'] = 'aws-us-west-2'\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-west-2' \n",
    "print(os.getenv('AWS_PROFILE'))\n",
    "\n",
    "session = boto3.Session(profile_name='aws-us-west-2')\n",
    "# Create an STS client\n",
    "sts = session.client('sts')\n",
    "\n",
    "# Get the current account ID\n",
    "account_id = sts.get_caller_identity()['Account']\n",
    "\n",
    "# Print the account ID\n",
    "print(f\"AWS Account ID: {account_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bucket for the images to be store\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Construct the bucket name using the AWS Account ID and a string\n",
    "bucketname = f\"{account_id}-vectorimagestore\"\n",
    "\n",
    "# Create an S3 resource\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "# Create the S3 bucket\n",
    "try:\n",
    "    bucket = s3.create_bucket(\n",
    "        Bucket=bucketname,\n",
    "        CreateBucketConfiguration={\n",
    "            'LocationConstraint': session.region_name\n",
    "        }\n",
    "    )\n",
    "    print(f\"Created S3 bucket: {bucketname}\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'BucketAlreadyOwnedByYou':\n",
    "        print(f\"Bucket {bucketname} already exists.\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# Create a CloudFront client\n",
    "cf = session.client('cloudfront')\n",
    "\n",
    "try:\n",
    "    # Create the origin access identity\n",
    "    response = cf.create_cloud_front_origin_access_identity(\n",
    "        CloudFrontOriginAccessIdentityConfig={\n",
    "            'CallerReference': str(hash(bucketname)),\n",
    "            'Comment': 'Access identity for ' + bucketname\n",
    "        }\n",
    "    )\n",
    "\n",
    "    origin_access_identity_id = response['CloudFrontOriginAccessIdentity']['Id']\n",
    "    origin_access_identity_arn = response['CloudFrontOriginAccessIdentity']['S3CanonicalUserId']\n",
    "\n",
    "    # Configure the S3 origin\n",
    "    origin = {\n",
    "        'Id': bucketname,\n",
    "        'DomainName': f'{bucketname}.s3.amazonaws.com',\n",
    "        'S3OriginConfig': {\n",
    "            'OriginAccessIdentity': f'origin-access-identity/cloudfront/{origin_access_identity_id}'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create the distribution configuration\n",
    "    distribution_config = {\n",
    "        'CallerReference': str(hash(bucketname)),\n",
    "        'Comment': 'Distribution for ' + bucketname,\n",
    "        'Origins': {\n",
    "            'Quantity': 1,\n",
    "            'Items': [origin]\n",
    "        },\n",
    "        'DefaultCacheBehavior': {\n",
    "            'TargetOriginId': bucketname,\n",
    "            'ViewerProtocolPolicy': 'redirect-to-https',\n",
    "            'AllowedMethods': {\n",
    "                'Quantity': 2,\n",
    "                'Items': ['GET', 'HEAD'],\n",
    "                'CachedMethods': {\n",
    "                    'Quantity': 2,\n",
    "                    'Items': ['GET', 'HEAD']\n",
    "                }\n",
    "            },\n",
    "            'Compress': True,\n",
    "            'DefaultTTL': 86400,\n",
    "            'MaxTTL': 31536000,\n",
    "            'MinTTL': 0,\n",
    "            'ForwardedValues': {\n",
    "                'QueryString': False,\n",
    "                'Cookies': {\n",
    "                    'Forward': 'none'\n",
    "                },\n",
    "                'Headers': {\n",
    "                    'Quantity': 0\n",
    "                },\n",
    "                'QueryStringCacheKeys': {\n",
    "                    'Quantity': 0\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        'Enabled': True\n",
    "    }\n",
    "\n",
    "    # Create the CloudFront distribution\n",
    "    response = cf.create_distribution(\n",
    "        DistributionConfig=distribution_config\n",
    "    )\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    # Print the distribution URL\n",
    "    distribution_url = response['Distribution']['DomainName']\n",
    "    print(f\"CloudFront distribution URL: https://{distribution_url}\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'DistributionAlreadyExists':\n",
    "        print(\"CloudFront distribution already exists.\")\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the image_catalog URL base path\n",
    "Update the json with the new CloudFront Distribution base URL stored in ```distribution_url```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Set the path to the image catalog file\n",
    "image_catalog_file = \"image_data/image_catalog.json\"\n",
    "\n",
    "# Read the existing JSON data from the file\n",
    "with open(image_catalog_file, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Update the \"image_url\" field with the CloudFront distribution URL\n",
    "for item in data:\n",
    "    item[\"image_url\"] = item[\"image_url\"].replace(\"cloudfrontURL\", distribution_url)\n",
    "\n",
    "# Write the updated data back to the file\n",
    "with open(image_catalog_file, \"w\") as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "print(\"Image catalog file updated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload image_data to S3 and push to CloudFront distribition\n",
    "Copy the data from ./image_data to S3 bucket\n",
    "Invalidate the distribution cache sending it to CloudFront"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# AWS credentials and region\n",
    "AWS_PROFILE = 'aws-us-west-2'\n",
    "AWS_REGION = 'us-west-2'\n",
    "\n",
    "# S3 bucket name\n",
    "bucketname = f\"{account_id}-vectorimagestore\"\n",
    "\n",
    "# CloudFront distribution ID\n",
    "CLOUDFRONT_DISTRIBUTION_ID = 'E2J2I9LLRVP1CK'\n",
    "\n",
    "# Create S3 and CloudFront clients\n",
    "session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "s3 = session.client('s3')\n",
    "cloudfront = session.client('cloudfront')\n",
    "\n",
    "# Upload files to S3\n",
    "def upload_to_s3(local_dir, s3_prefix):\n",
    "    for root, dirs, files in os.walk(local_dir):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            s3_path = os.path.join(s3_prefix, os.path.relpath(local_path, local_dir))\n",
    "            try:\n",
    "                s3.upload_file(local_path, bucketname, s3_path)\n",
    "                print(f\"Uploaded {local_path} to s3://{bucketname}/{s3_path}\")\n",
    "            except ClientError as e:\n",
    "                print(f\"Error uploading {local_path}: {e}\")\n",
    "\n",
    "# Invalidate CloudFront distribution cache\n",
    "def invalidate_cloudfront_cache(paths):\n",
    "    try:\n",
    "        response = cloudfront.create_invalidation(\n",
    "            DistributionId=CLOUDFRONT_DISTRIBUTION_ID,\n",
    "            InvalidationBatch={\n",
    "                'Paths': {\n",
    "                    'Quantity': len(paths),\n",
    "                    'Items': paths\n",
    "                },\n",
    "                'CallerReference': 'unique-reference-id'\n",
    "            }\n",
    "        )\n",
    "        print(f\"Invalidated CloudFront distribution cache for {', '.join(paths)}\")\n",
    "    except ClientError as e:\n",
    "        print(f\"Error invalidating CloudFront distribution cache: {e}\")\n",
    "\n",
    "# Upload image data to S3\n",
    "upload_to_s3('./image_data', '')\n",
    "\n",
    "# Invalidate CloudFront distribution cache\n",
    "invalidate_cloudfront_cache(['/*'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Bucket Policy for CloudFront Distribution\n",
    "The policy provides access for CloudFront to the objects stored in the bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** NOT COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bucket policy to allow CloudFront access\n",
    "print(f\"Creating bucket policy for CloudFront Origin Access Identity: {CLOUDFRONT_DISTRIBUTION_ID}\")\n",
    "\n",
    "# Get the CloudFront Origin Access Identity (OAI)\n",
    "try:\n",
    "    response = cloudfront.get_cloud_front_origin_access_identity_config(\n",
    "        Id=CLOUDFRONT_DISTRIBUTION_ID\n",
    "    )\n",
    "    oai_arn = response['CloudFrontOriginAccessIdentityConfig']['S3CanonicalUserId']\n",
    "except ClientError as e:\n",
    "    print(f\"Error getting CloudFront OAI: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"CloudFront Response: {response}\")\n",
    "\n",
    "bucket_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"AWS\": f\"arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity {CLOUDFRONT_DISTRIBUTION_ID}\"\n",
    "            },\n",
    "            \"Action\": \"s3:GetObject\",\n",
    "            \"Resource\": f\"arn:aws:s3:::{bucketname}/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    s3.put_bucket_policy(Bucket=bucketname, Policy=json.dumps(bucket_policy))\n",
    "    print(f\"Bucket policy created for {bucketname}\")\n",
    "except ClientError as e:\n",
    "    print(f\"Error creating bucket policy: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"Version\": \"2008-10-17\",\n",
    "    \"Id\": \"PolicyForCloudFrontPrivateContent\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"1\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"AWS\": \"arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity E3BVAIXSO0D205\"\n",
    "            },\n",
    "            \"Action\": \"s3:GetObject\",\n",
    "            \"Resource\": \"arn:aws:s3:::682723651788-vectorimagestore/*\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings for image data and store embeddings in local JSON files\n",
    "1. Initialize the boto3 client for Amazon Bedrock to use the Bedrock API's for invoking the Amazon Titan Multimodal Embeddings model\n",
    "2. Generate embeddings for the images and their corresponding product titles in a shared embedding space by invoking the Amazon Titan Multimodal Embeddings Model on the tourism dataset represented in ./image_data/image_catalog.json files\n",
    "3. Save the generated embeddings in flat JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "(code removed) After the above code snippet is executed, here is the structure of the embedding files generated in my local directory. These embeddings represent the vector representations of the Indo-fashion images and their product titles in a shared embedding space. Each vector representation generated by the Amazon Titan Multimodal Embeddings model has 1024 dimensions which is the default value, but other dimensions are available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ```\n",
    " \n",
    " [\n",
    "  {\n",
    "    \"image_path\": \"001.jpg\",\n",
    "    \"image_title\": \"Driver Career\",\n",
    "    \"image_labels\": \"Bus Driver Transport Green\",\n",
    "    \"image_class\": \"tower\",\n",
    "    \"image_url\": \"https://d9yx5bzoplulh.cloudfront.net/001.jpg\",\n",
    "    \"multimodal_vector\": [\n",
    "      0.043503903,\n",
    "      -0.011784887,\n",
    "      -0.021839952,\n",
    "      -0.014455186,\n",
    "      ........\n",
    "      ........\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"image_path\": \"002.jpg\",\n",
    "    \"image_title\": \"Beach\",\n",
    "    \"image_labels\": \"Beach Ocean Sand Swimming\",\n",
    "    \"image_class\": \"sealink\",\n",
    "    \"image_url\": \"https://d9yx5bzoplulh.cloudfront.net/002.jpg\",\n",
    "    \"multimodal_vector\": [\n",
    "      0.014641403,\n",
    "      0.017837116,\n",
    "      0.0024839255,\n",
    "      -0.013209195,\n",
    "      0.021653477,\n",
    "      0.015551946,\n",
    "      ........\n",
    "      ........\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the OpenSearch serverless collection\n",
    "\n",
    "Amazon OpenSearch Serverless has a new feature called Collections that I use below for storing the vector embeddings generated above. I initialized an Amazon OpenSearch Serverless Collection named “image-search-multimodal” as shown below. Additionally, I create the encryption policies, network policies, and data access policies for accessing the Amazon OpenSearch Serverless Collection. At the end of the code block below, I instantiate an Amazon OpenSearch Serverless client to connect to Amazon OpenSearch Serverless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the opensearch serverless collection\n",
    "# create the opensearch client\n",
    "\n",
    "import boto3\n",
    "import time\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "os.environ['AWS_PROFILE'] = 'aws-us-west-2'\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-west-2' \n",
    "print(os.getenv('AWS_PROFILE'))\n",
    "\n",
    "session = boto3.Session(profile_name='aws-us-west-2')\n",
    "\n",
    "vector_store_name = 'image-search-multimodal'\n",
    "index_name = \"image-search-multimodal-index\"\n",
    "encryption_policy_name = \"image-search-multimodal-ep\"\n",
    "network_policy_name = \"image-search-multimodal-np\"\n",
    "access_policy_name = 'image-search-multimodal-ap'\n",
    "identity = session.client('sts').get_caller_identity()['Arn']\n",
    "\n",
    "#aoss_client = boto3.client('opensearchserverless')\n",
    "aoss_client = session.client('opensearchserverless')\n",
    "\n",
    "security_policy = aoss_client.create_security_policy(\n",
    "    name = encryption_policy_name,\n",
    "    policy = json.dumps(\n",
    "        {\n",
    "            'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "            'ResourceType': 'collection'}],\n",
    "            'AWSOwnedKey': True\n",
    "        }),\n",
    "    type = 'encryption'\n",
    ")\n",
    "\n",
    "network_policy = aoss_client.create_security_policy(\n",
    "    name = network_policy_name,\n",
    "    policy = json.dumps(\n",
    "        [\n",
    "            {'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "            'ResourceType': 'collection'}],\n",
    "            'AllowFromPublic': True}\n",
    "        ]),\n",
    "    type = 'network'\n",
    ")\n",
    "\n",
    "collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')\n",
    "\n",
    "while True:\n",
    "    status = aoss_client.list_collections(collectionFilters={'name':vector_store_name})['collectionSummaries'][0]['status']\n",
    "    if status in ('ACTIVE', 'FAILED'): break\n",
    "    time.sleep(10)\n",
    "\n",
    "access_policy = aoss_client.create_access_policy(\n",
    "    name = access_policy_name,\n",
    "    policy = json.dumps(\n",
    "        [\n",
    "            {\n",
    "                'Rules': [\n",
    "                    {\n",
    "                        'Resource': ['collection/' + vector_store_name],\n",
    "                        'Permission': [\n",
    "                            'aoss:CreateCollectionItems',\n",
    "                            'aoss:DeleteCollectionItems',\n",
    "                            'aoss:UpdateCollectionItems',\n",
    "                            'aoss:DescribeCollectionItems'],\n",
    "                        'ResourceType': 'collection'\n",
    "                    },\n",
    "                    {\n",
    "                        'Resource': ['index/' + vector_store_name + '/*'],\n",
    "                        'Permission': [\n",
    "                            'aoss:CreateIndex',\n",
    "                            'aoss:DeleteIndex',\n",
    "                            'aoss:UpdateIndex',\n",
    "                            'aoss:DescribeIndex',\n",
    "                            'aoss:ReadDocument',\n",
    "                            'aoss:WriteDocument'],\n",
    "                        'ResourceType': 'index'\n",
    "                    }],\n",
    "                'Principal': [identity],\n",
    "                'Description': 'Easy data policy'}\n",
    "        ]),\n",
    "    type = 'data'\n",
    ")\n",
    "\n",
    "host = collection['createCollectionDetail']['id'] + '.' + os.environ.get(\"AWS_DEFAULT_REGION\", None) + '.aoss.amazonaws.com:443'\n",
    "print(host)\n",
    "service = 'aoss'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, os.environ.get(\"AWS_DEFAULT_REGION\", None), service)\n",
    "host_parts = host.split(':')\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': host_parts[0], 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    pool_maxsize = 20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the vector index\n",
    "Amazon OpenSearch is a versatile and fully managed suite for search and analytics, that offers robust scalability. It supports KNN (K-Nearest Neighbors) search, enabling retrieval of similar documents based on vectors. To use this capability, I create a new vector index within the vector search collection. Each document within this index will encompass six key properties: \"image_path,\" \"image_title,\" \"image_labels,\" \"image_class,\" \"image_url,\" and a vector embeddings field named \"multimodal_vector\". The dense vector embeddings, generated from the Indo Fashion dataset by the Amazon Titan Multimodal Embeddings model, will contain 1024 dimensions. To query these vectors, the index \"image-search-multimodal-index\" is configured to use the Non-Metric Space Library (nmslib), with the Hierarchical Navigable Small Worlds algorithm (HNSW) and cosine similarity (cosinesimil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the vector index\n",
    "\n",
    "import json\n",
    "index_name = \"image-search-multimodal-index\" \n",
    "\n",
    "index_body = {\n",
    "    \"mappings\": { \n",
    "        \"properties\": {\n",
    "            \"image_path\": {\"type\": \"text\"}, \n",
    "            \"image_title\": {\"type\": \"text\"}, \n",
    "            \"image_labels\": {\"type\": \"text\"}, \n",
    "            \"image_class\": {\"type\": \"text\"}, \n",
    "            \"image_url\": {\"type\": \"text\"}, \n",
    "            \"multimodal_vector\": {\n",
    "                \"type\": \"knn_vector\", \n",
    "                \"dimension\": 1024, \n",
    "                \"method\": \n",
    "                {\n",
    "                    \"engine\": \"nmslib\",\n",
    "                    \"space_type\": \"cosinesimil\",\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"parameters\": {\"ef_construction\": 512, \"m\": 16},\n",
    "                }, \n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"number_of_shards\": 2, \n",
    "            \"knn.algo_param\": {\"ef_search\": 512}, \n",
    "            \"knn\": True,\n",
    "        } \n",
    "    },\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = client.indices.create(index_name, body=index_body) \n",
    "    print(json.dumps(response, indent=2))\n",
    "except Exception as ex: print(ex)\n",
    "\n",
    "# describe new vector index\n",
    "try:\n",
    "    response = client.indices.get(\"image-search-multimodal-index\") \n",
    "    print(json.dumps(response, indent=2))\n",
    "except Exception as ex: print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "You can also review the new Vectorsearch collection, \"image-search-multimodal\", and the associated vector index, \"image-search-multimodal-index\", from the ([Amazon OpenSearch Service console](https://console.aws.amazon.com/aos/home/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read embeddings from flat files and insert them into Amazon OpenSearch\n",
    "In the next code snippet below, I ingest the JSON Documents generated in Step 2 into the Amazon OpenSearch index \"image-search-multimodal-index\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read embeddings from flat file and insert into opensearch\n",
    "\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from opensearchpy import helpers\n",
    "from opensearchpy.helpers import bulk\n",
    "\n",
    "# Directory containing the JSON files\n",
    "json_files_directory = \"./\"\n",
    "\n",
    "# Iterate through each JSON file\n",
    "for filename in os.listdir(json_files_directory):\n",
    "    if filename.startswith(\"embedding_requests\") and filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(json_files_directory, filename)\n",
    "\n",
    "        # Load JSON file\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        print(f\"Starting indexing for :: {filename}\")\n",
    "\n",
    "        # Use the bulk API to insert documents for the current file\n",
    "        success, failed = bulk(\n",
    "            client,\n",
    "            data,\n",
    "            index=\"image-search-multimodal-index\",  # Replace with your OpenSearch index name\n",
    "            raise_on_exception=True\n",
    "        )\n",
    "\n",
    "        print(f\"Indexed {success} documents successfully, {failed} documents failed for file: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the documents are ingested into the index, the Amazon OpenSearch Collections index **product-search-mulitmodal-index** looks like:\n",
    "\n",
    "**Vector Fields**\n",
    "* Vector Field Name: multimodal_vector\n",
    "* Engine: nmslib\n",
    "* Dimensions: 1024\n",
    "* Distance Type: cosine\n",
    "* M: 16\n",
    "* ef_construction: 512\n",
    "* ef_search: 512\n",
    "\n",
    "\n",
    "**Metadata**\n",
    "* image_labels\n",
    "* image_class\n",
    "* image_path\n",
    "* image_title\n",
    "* image_url\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
